# ğŸ§¹ GPU Memory Optimization Guide

## ğŸš¨ Problem Analysis

Logi pokazujÄ… **CUDA Out of Memory** bÅ‚Ä™dy:
```
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 72.00 MiB. 
GPU 0 has a total capacity of 31.36 GiB of which 49.00 MiB is free. 
Process 623 has 18.63 GiB memory in use.
```

**GÅ‚Ã³wny problem**: GPU ma 31GB pamiÄ™ci, ale prawie wszystka jest zajÄ™ta (tylko 49MB wolne).

## ğŸ”§ RozwiÄ…zania

### 1. **UÅ¼yj Zoptymalizowanej Konfiguracji**

**Nowy plik**: `training_memory_optimized.yaml`

**Kluczowe zmiany**:
```yaml
# Drastyczne redukcje pamiÄ™ci:
linear: 8                    # ByÅ‚o: 20 â†’ TERAZ: 8 (-60% RAM)
resolution: [512]            # ByÅ‚o: [512, 768, 1024] â†’ TERAZ: tylko 512
low_vram: true              # ByÅ‚o: false â†’ WÅÄ„CZONE
dtype: "fp16"               # ByÅ‚o: "bf16" â†’ fp16 (-20% RAM)
enable_sequential_cpu_offload: true  # NOWE - CPU offload
enable_attention_slicing: true       # NOWE - slice attention
pin_memory: false           # ByÅ‚o: true â†’ WYÅÄ„CZONE
num_workers: 1              # ByÅ‚o: 2 â†’ ZREDUKOWANE
```

### 2. **WyczyÅ›Ä‡ GPU Memory**

```bash
cd Serverless/Backend
python3 cleanup_gpu_memory.py
```

Ten skrypt:
- âœ… Sprawdzi status GPU i procesÃ³w
- âœ… Znajdzie procesy AI toolkit (stuck training)
- âœ… Pozwoli bezpiecznie zabiÄ‡ procesy
- âœ… WyczyÅ›ci CUDA cache

### 3. **PorÃ³wnanie Konfiguracji**

| Ustawienie | OryginaÅ‚ | Optymalizowana | OszczÄ™dnoÅ›Ä‡ RAM |
|------------|----------|----------------|-----------------|
| **LoRA Rank** | 20 | 8 | ~60% |
| **Resolution** | [512,768,1024] | [512] | ~70% |
| **Precision** | bf16 | fp16 | ~20% |
| **CPU Offload** | âŒ | âœ… | ~40% |
| **Attention Slice** | âŒ | âœ… | ~30% |
| **Pin Memory** | âœ… | âŒ | ~10% |
| **Workers** | 2 | 1 | ~15% |

**Szacunkowe oszczÄ™dnoÅ›ci**: **~70-80% pamiÄ™ci GPU** 

## ğŸš€ Instrukcje WdroÅ¼enia

### Krok 1: WyczyÅ›Ä‡ GPU
```bash
python3 cleanup_gpu_memory.py
# Zabij stuck processes gdy zapyta
```

### Krok 2: UÅ¼yj nowej konfiguracji
```bash
# W fronendzie lub API call:
{
  "job_type": "train_with_yaml",
  "yaml_config": "< zawartoÅ›Ä‡ training_memory_optimized.yaml >"
}
```

### Krok 3: Monitoruj pamiÄ™Ä‡
```bash
# SprawdÅº status GPU w czasie rzeczywistym:
watch -n 2 'nvidia-smi --query-gpu=memory.used,memory.free --format=csv'
```

## ğŸ“Š Oczekiwane Rezultaty

**Przed optymalizacjÄ…**:
- âŒ GPU Memory: 31GB uÅ¼ywane / 31GB total (99%)
- âŒ Free: ~49MB
- âŒ Training: CRASH - Out of Memory

**Po optymalizacji**:
- âœ… GPU Memory: ~8-12GB uÅ¼ywane / 31GB total (30-40%)  
- âœ… Free: ~19-23GB
- âœ… Training: SUKCES - Stabilne uruchomienie

## âš™ï¸ Dodatkowe Optymalizacje

JeÅ›li nadal problemy z pamiÄ™ciÄ…:

1. **Jeszcze mniejszy LoRA rank**: `linear: 4`
2. **Mniejsza rozdzielczoÅ›Ä‡**: `resolution: [256]` 
3. **Gradient accumulation**: `gradient_accumulation_steps: 4`
4. **WyÅ‚Ä…cz sampling**: `sample_every: 0`

## ğŸ” Debug Commands

```bash
# SprawdÅº pamiÄ™Ä‡ GPU:
nvidia-smi

# SprawdÅº procesy GPU:
nvidia-smi pmon

# SprawdÅº szczegÃ³Å‚y procesÃ³w:
ps aux | grep python

# Monitor w czasie rzeczywistym:
watch -n 1 'nvidia-smi --query-gpu=memory.used,memory.free,utilization.gpu --format=csv'
```

## ğŸ†˜ JeÅ›li Nadal Problemy

1. **Restart kontenera RunPod**
2. **UÅ¼yj wiÄ™kszego GPU** (A100 zamiast A40)
3. **Gradient checkpointing**: juÅ¼ wÅ‚Ä…czone
4. **DeepSpeed Zero**: dla bardzo duÅ¼ych modeli

**Kontakt**: JeÅ›li optymalizacje nie pomagajÄ…, sprawdÅº logi i status GPU po cleanup.