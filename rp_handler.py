#!/usr/bin/env python3
"""
üöÄ ULTRA-FAST RUNPOD HANDLER - LoRA Dashboard Backend
Minimal handler with runtime setup for heavy dependencies
Deploy time: ~30 seconds instead of 20 minutes!
Based on successful runpod-fastbackend/ approach

üîÑ QUEUE MANAGEMENT:
- Uses RunPod's built-in queue system (IN_QUEUE ‚Üí IN_PROGRESS ‚Üí COMPLETED)
- No Redis needed - RunPod handles job queuing and status tracking
- Handler simply returns results, RunPod manages the rest
"""

import runpod
import asyncio
import logging
import os
import sys
import subprocess
import time
from datetime import datetime
from typing import Dict, Any

# Global flag to track if environment is setup
ENVIRONMENT_READY = False
SETUP_LOCK = False

def log(message, level="INFO"):
    """Unified logging to stdout and stderr for RunPod visibility"""
    timestamp = datetime.now().strftime("%H:%M:%S")
    log_msg = f"[{timestamp}] {level}: {message}"
    
    # Write to both stdout and stderr for maximum visibility
    print(log_msg)
    sys.stderr.write(f"{log_msg}\n")
    sys.stderr.flush()
    sys.stdout.flush()

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def setup_environment():
    """Simplified setup for fast deployment"""
    global ENVIRONMENT_READY, SETUP_LOCK
    
    if ENVIRONMENT_READY:
        log("Environment already ready", "INFO")
        return True
        
    if SETUP_LOCK:
        log("Environment setup in progress...", "WARN")
        # Wait for setup to complete
        while SETUP_LOCK and not ENVIRONMENT_READY:
            time.sleep(1)
        return ENVIRONMENT_READY
    
    SETUP_LOCK = True
    
    try:
        log("üöÄ Setting up minimal environment...", "INFO")
        
        # Step 1: Setup directories
        log("üìÅ Creating workspace directories...", "INFO")
        os.makedirs("/workspace", exist_ok=True)
        os.makedirs("/workspace/training_data", exist_ok=True)
        os.makedirs("/workspace/models", exist_ok=True)
        os.makedirs("/workspace/logs", exist_ok=True)
        
        # Step 2: Optional HuggingFace token (non-blocking)
        hf_token = os.environ.get("HF_TOKEN", "")
        if hf_token and hf_token != "":
            log("ü§ó HuggingFace token found, continuing...", "INFO")
        else:
            log("‚ÑπÔ∏è No HuggingFace token provided", "INFO")
        
        # Step 3: Install minimal dependencies (no Redis - RunPod has built-in queue)
        log("üì¶ Installing minimal dependencies...", "INFO")
        try:
            result = subprocess.run([
                sys.executable, "-m", "pip", "install", 
                "boto3>=1.34.0"  # Only S3 for storage, RunPod handles queue
            ], capture_output=True, text=True, timeout=60)
            
            if result.returncode == 0:
                log("‚úÖ Minimal dependencies installed", "INFO")
            else:
                log("‚ö†Ô∏è Some dependencies failed, continuing...", "WARN")
        except Exception as e:
            log(f"‚ö†Ô∏è Dependency install warning: {e}, continuing...", "WARN")
        
        log("‚úÖ Minimal environment ready!", "INFO")
        ENVIRONMENT_READY = True
        return True
        
    except Exception as e:
        log(f"‚ùå Setup error: {e}", "ERROR")
        return False
    finally:
        SETUP_LOCK = False

def get_mock_services():
    """Return mock services for simplified deployment"""
    log("Using simplified mock services", "INFO")
    
    # Mock classes with basic methods
    class MockGPUManager:
        def __init__(self):
            log("Mock GPU Manager initialized", "INFO")
        
        def get_status(self):
            return {"status": "healthy", "gpus": 1, "memory": "24GB"}
    
    class MockProcessManager:
        def __init__(self, **kwargs):
            log("Mock Process Manager initialized", "INFO")
            log("Using RunPod built-in queue system (no Redis needed)", "INFO")
            self.processes = {}  # Local storage for demo - RunPod handles real queue
        
        async def initialize(self):
            log("Mock Process Manager initialized async", "INFO")
        
        async def start_training(self, config):
            process_id = f"mock_train_{int(time.time())}"
            self.processes[process_id] = {"status": "completed", "type": "training"}
            log(f"Mock training started: {process_id}", "INFO")
            return process_id
        
        async def start_generation(self, config):
            process_id = f"mock_gen_{int(time.time())}"
            self.processes[process_id] = {"status": "completed", "type": "generation"}
            log(f"Mock generation started: {process_id}", "INFO")
            return process_id
        
        async def get_process(self, process_id):
            return self.processes.get(process_id, {"status": "not_found"})
        
        async def cancel_process(self, process_id):
            if process_id in self.processes:
                self.processes[process_id]["status"] = "cancelled"
                return True
            return False
        
        async def list_processes(self, **kwargs):
            return list(self.processes.values())
    
    class MockStorageService:
        def __init__(self):
            log("Mock Storage Service initialized", "INFO")
        
        async def health_check(self):
            return "healthy"
        
        async def get_download_url(self, process_id):
            return f"https://mock-storage.com/download/{process_id}"
        
        async def list_files(self, path):
            return [{"key": f"{path}/mock_file.txt", "size": 1024}]
    
    class MockLoRAService:
        def __init__(self, storage=None):
            log("Mock LoRA Service initialized", "INFO")
        
        async def get_available_models(self):
            return ["mock_lora_model_1.safetensors", "mock_lora_model_2.safetensors"]
    
    return {
        'GPUManager': MockGPUManager,
        'ProcessManager': MockProcessManager,
        'StorageService': MockStorageService,
        'LoRAService': MockLoRAService,
        'get_settings': lambda: {"workspace_path": "/workspace"}
    }

# Global service instances (initialized on first use)
_services_initialized = False
_services = None
_gpu_manager = None
_process_manager = None
_storage_service = None
_lora_service = None

async def initialize_services():
    """Initialize simplified services for fast deployment"""
    global _services_initialized, _services, _gpu_manager, _process_manager, _storage_service, _lora_service
    
    if _services_initialized:
        return
    
    try:
        # First setup environment
        if not setup_environment():
            log("‚ö†Ô∏è Environment setup failed, using minimal mode", "WARN")
        
        # Use mock services for simplified deployment
        _services = get_mock_services()
        
        log("üöÄ Initializing simplified services...", "INFO")
        
        settings = _services['get_settings']() if _services['get_settings'] else {"workspace_path": "/workspace"}
        
        # Initialize mock services (create instances)
        _storage_service = _services['StorageService']()
        _lora_service = _services['LoRAService'](_storage_service) 
        _gpu_manager = _services['GPUManager']()
        _process_manager = _services['ProcessManager'](
            gpu_manager=_gpu_manager,
            storage_service=_storage_service
            # No Redis needed - RunPod handles queue management
        )
        
        # Initialize process manager async
        if _process_manager:
            await _process_manager.initialize()
        
        _services_initialized = True
        log("‚úÖ Simplified services ready!", "INFO")
        
    except Exception as e:
        log(f"‚ùå Failed to initialize services: {e}", "ERROR")
        # Don't raise - continue with minimal functionality
        _services_initialized = True
        log("‚ö†Ô∏è Continuing with minimal functionality", "WARN")

async def async_handler(event: Dict[str, Any]) -> Dict[str, Any]:
    """
    Main RunPod Serverless handler function
    
    Expected input format:
    {
        "input": {
            "type": "train" | "generate" | "health" | "processes" | "lora",
            "config": "YAML configuration string",
            "process_id": "optional process ID for status check"
        }
    }
    """
    request_id = None
    try:
        # Initialize services if needed
        await initialize_services()
        
        job_input = event.get("input", {})
        job_type = job_input.get("type")
        
        # Handle simple prompt requests (auto-detect as generation)
        if not job_type and job_input.get("prompt"):
            job_type = "generate"
            log("üìù Auto-detected generation request from prompt", "INFO")
        
        # Log incoming request
        request_id = f"req_{int(time.time() * 1000)}"
        log(f"üì® Incoming {job_type or 'unknown'} request | Request ID: {request_id}", "INFO")
        
        log(f"üì® Processing job type: {job_type} | Request ID: {request_id}", "INFO")
        
        if job_type == "health":
            response = await handle_health_check()
        elif job_type == "train":
            response = await handle_training(job_input)
        elif job_type == "generate":
            response = await handle_generation(job_input)
        elif job_type == "processes":
            response = await handle_get_processes(job_input)
        elif job_type == "process_status":
            response = await handle_process_status(job_input)
        elif job_type == "lora":
            response = await handle_get_lora_models()
        elif job_type == "cancel":
            response = await handle_cancel_process(job_input)
        elif job_type == "download":
            response = await handle_download_url(job_input)
        elif job_type == "upload_training_data":
            response = await handle_upload_training_data(job_input, request_id)
        elif job_type == "bulk_download":
            response = await handle_bulk_download(job_input)
        else:
            response = {
                "error": f"Unknown job type: {job_type}",
                "supported_types": ["health", "train", "generate", "processes", "process_status", "lora", "cancel", "download", "upload_training_data", "bulk_download"]
            }
        
        # Log successful response
        status = "success" if not response.get("error") else "error"
        log(f"‚úÖ Request completed: {status} | Request ID: {request_id}", "INFO")
        
        return response
            
    except Exception as e:
        log(f"üí• Handler error: {e}", "ERROR")
        error_response = {"error": str(e)}
        
        # Log error response
        log(f"‚ùå Request failed | Request ID: {request_id} | Error: {str(e)}", "ERROR")
        
        return error_response

async def handle_health_check() -> Dict[str, Any]:
    """Health check endpoint"""
    try:
        services_status = {}
        
        if _process_manager:
            services_status["process_manager"] = "healthy"
        if _storage_service:
            services_status["storage"] = await _storage_service.health_check()
        if _gpu_manager:
            services_status["gpu_manager"] = _gpu_manager.get_status()
            
        return {
            "status": "healthy",
            "services": services_status,
            "worker_id": os.environ.get("RUNPOD_WORKER_ID", "local"),
            "environment": "serverless"
        }
    except Exception as e:
        return {"error": f"Health check failed: {str(e)}"}

async def handle_training(job_input: Dict[str, Any]) -> Dict[str, Any]:
    """Handle training request"""
    try:
        config = job_input.get("config")
        if not config:
            return {"error": "Missing 'config' parameter"}
        
        if not _process_manager:
            return {"error": "Process manager not initialized"}
        
        process_id = await _process_manager.start_training(config)
        log(f"‚úÖ Training started with process_id: {process_id}", "INFO")
        
        return {"process_id": process_id}
    except Exception as e:
        log(f"‚ùå Training error: {e}", "ERROR")
        return {"error": f"Failed to start training: {str(e)}"}

async def handle_generation(job_input: Dict[str, Any]) -> Dict[str, Any]:
    """Handle generation request"""
    try:
        # Support both config and simple prompt
        config = job_input.get("config")
        prompt = job_input.get("prompt")
        
        if not config and not prompt:
            return {"error": "Missing 'config' or 'prompt' parameter"}
        
        # Create simple config from prompt if needed
        if not config and prompt:
            config = f"prompt: '{prompt}'"
            log(f"üìù Created simple config from prompt: {prompt}", "INFO")
        
        if not _process_manager:
            return {"error": "Process manager not initialized"}
        
        process_id = await _process_manager.start_generation(config)
        log(f"‚úÖ Generation started with process_id: {process_id}", "INFO")
        
        return {
            "process_id": process_id,
            "message": f"Generation started for prompt: {prompt}" if prompt else "Generation started",
            "status": "started"
        }
    except Exception as e:
        log(f"‚ùå Generation error: {e}", "ERROR")
        return {"error": f"Failed to start generation: {str(e)}"}

async def handle_get_processes(job_input: Dict[str, Any]) -> Dict[str, Any]:
    """Handle get processes request"""
    try:
        if not _process_manager:
            return {"error": "Process manager not initialized"}
        
        processes = await _process_manager.get_all_processes()
        return {"processes": processes}
    except Exception as e:
        log(f"‚ùå Get processes error: {e}", "ERROR")
        return {"error": f"Failed to get processes: {str(e)}"}

async def handle_process_status(job_input: Dict[str, Any]) -> Dict[str, Any]:
    """Handle process status request"""
    try:
        process_id = job_input.get("process_id")
        if not process_id:
            return {"error": "Missing 'process_id' parameter"}
        
        if not _process_manager:
            return {"error": "Process manager not initialized"}
        
        process = await _process_manager.get_process(process_id)
        if not process:
            return {"error": "Process not found"}
            
        return process
    except Exception as e:
        log(f"‚ùå Process status error: {e}", "ERROR")
        return {"error": f"Failed to get process status: {str(e)}"}

async def handle_get_lora_models() -> Dict[str, Any]:
    """Handle get LoRA models request"""
    try:
        if not _lora_service:
            return {"error": "LoRA service not initialized"}
        
        models = await _lora_service.get_available_models()
        return {"models": models}
    except Exception as e:
        log(f"‚ùå Get LoRA models error: {e}", "ERROR")
        return {"error": f"Failed to get LoRA models: {str(e)}"}

async def handle_cancel_process(job_input: Dict[str, Any]) -> Dict[str, Any]:
    """Handle cancel process request"""
    try:
        process_id = job_input.get("process_id")
        if not process_id:
            return {"error": "Missing 'process_id' parameter"}
        
        if not _process_manager:
            return {"error": "Process manager not initialized"}
        
        success = await _process_manager.cancel_process(process_id)
        if not success:
            return {"error": "Process not found or cannot be cancelled"}
            
        log(f"‚úÖ Process {process_id} cancelled successfully", "INFO")
        return {"message": "Process cancelled successfully"}
    except Exception as e:
        log(f"‚ùå Cancel process error: {e}", "ERROR")
        return {"error": f"Failed to cancel process: {str(e)}"}

async def handle_download_url(job_input: Dict[str, Any]) -> Dict[str, Any]:
    """Handle download URL request"""
    try:
        process_id = job_input.get("process_id")
        if not process_id:
            return {"error": "Missing 'process_id' parameter"}
        
        if not _process_manager:
            return {"error": "Process manager not initialized"}
        
        # Check if process exists and is completed
        process = await _process_manager.get_process(process_id)
        if not process:
            return {"error": "Process not found"}
            
        if process.status != "completed":
            return {"error": "Process not completed"}
        
        if not _storage_service:
            return {"error": "Storage service not initialized"}
            
        # Get download URL
        url = await _storage_service.get_download_url(process_id)
        
        return {"url": url}
    except Exception as e:
        log(f"‚ùå Download URL error: {e}", "ERROR")
        return {"error": f"Failed to get download URL: {str(e)}"}

async def handle_upload_training_data(job_input: Dict[str, Any], request_id: str = None) -> Dict[str, Any]:
    """Handle training data upload request"""
    try:
        import os
        import shutil
        import uuid
        from datetime import datetime
        
        # Get upload parameters from job input
        training_name = job_input.get("training_name", f"training_{int(datetime.now().timestamp())}")
        trigger_word = job_input.get("trigger_word", "")
        cleanup_existing = job_input.get("cleanup_existing", True)
        files_data = job_input.get("files", [])
        
        if not files_data:
            return {"error": "No files provided"}
        
        # Create unique training folder
        training_id = str(uuid.uuid4())[:8]
        safe_name = "".join(c if c.isalnum() or c in '-_' else '_' for c in training_name)
        workspace_path = os.environ.get("WORKSPACE_PATH", "/workspace")
        training_folder = os.path.join(workspace_path, "training_data", f"{safe_name}_{training_id}")
        
        # Clean up existing training data if requested
        if cleanup_existing:
            base_training_path = os.path.join(workspace_path, "training_data")
            if os.path.exists(base_training_path):
                for item in os.listdir(base_training_path):
                    old_path = os.path.join(base_training_path, item)
                    if os.path.isdir(old_path):
                        shutil.rmtree(old_path)
                        logger.info(f"Cleaned up old training data: {old_path}")
        
        # Create training folder
        os.makedirs(training_folder, exist_ok=True)
        
        uploaded_files = []
        image_count = 0
        caption_count = 0
        
        # Process files (expecting base64 encoded files)
        for file_info in files_data:
            filename = file_info.get("filename")
            content = file_info.get("content")  # base64 encoded
            content_type = file_info.get("content_type", "application/octet-stream")
            
            if not filename or not content:
                continue
                
            # Save file to training folder
            file_path = os.path.join(training_folder, filename)
            
            # Decode base64 content and save
            import base64
            try:
                file_content = base64.b64decode(content)
                with open(file_path, "wb") as f:
                    f.write(file_content)
                
                file_data = {
                    "filename": filename,
                    "path": file_path,
                    "size": len(file_content),
                    "content_type": content_type,
                    "uploaded_at": datetime.now().isoformat()
                }
                uploaded_files.append(file_data)
                
                # Log file operation
                log(f"üìÅ File uploaded: {filename} | Size: {file_data['size']} bytes | Request ID: {request_id}", "INFO")
                
                # Count file types
                if content_type and content_type.startswith('image/'):
                    image_count += 1
                elif filename.endswith('.txt'):
                    caption_count += 1
                    
            except Exception as e:
                log(f"‚ùå Failed to process file {filename} | Request ID: {request_id} | Error: {e}", "ERROR")
                continue
        
        # Create trigger word info file
        trigger_file = os.path.join(training_folder, "_training_info.txt")
        with open(trigger_file, "w") as f:
            f.write(f"Training Name: {training_name}\n")
            f.write(f"Trigger Word: {trigger_word}\n")
            f.write(f"Upload Date: {datetime.now().isoformat()}\n")
            f.write(f"Total Images: {image_count}\n")
            f.write(f"Total Captions: {caption_count}\n")
        
        response_data = {
            "uploaded_files": uploaded_files,
            "training_folder": training_folder,
            "total_images": image_count,
            "total_captions": caption_count,
            "message": f"Successfully uploaded {len(uploaded_files)} files to {training_folder}"
        }
        
        log(f"‚úÖ Training data uploaded: {training_folder} ({image_count} images, {caption_count} captions)", "INFO")
        
        return response_data
        
    except Exception as e:
        log(f"‚ùå Upload training data error: {e}", "ERROR")
        return {"error": f"Failed to upload training data: {str(e)}"}

async def handle_bulk_download(job_input: Dict[str, Any]) -> Dict[str, Any]:
    """Handle bulk download request"""
    try:
        process_ids = job_input.get("process_ids", [])
        include_images = job_input.get("include_images", True)
        include_loras = job_input.get("include_loras", True)
        
        if not process_ids:
            return {"error": "No process IDs provided"}
        
        if not _storage_service:
            return {"error": "Storage service not initialized"}
        
        download_items = []
        total_size = 0
        
        for process_id in process_ids:
            try:
                # List files for this process
                files = await _storage_service.list_files(f"results/{process_id}/")
                
                for file_info in files:
                    file_type = "other"
                    if any(ext in file_info.get('key', '').lower() for ext in ['.png', '.jpg', '.jpeg', '.webp']):
                        if include_images:
                            file_type = "image"
                        else:
                            continue
                    elif any(ext in file_info.get('key', '').lower() for ext in ['.safetensors', '.ckpt', '.pt']):
                        if include_loras:
                            file_type = "lora"
                        else:
                            continue
                    
                    # Generate download URL
                    download_url = await _storage_service.get_download_url(process_id)
                    if download_url:
                        download_items.append({
                            "filename": os.path.basename(file_info.get('key', '')),
                            "url": download_url,
                            "size": file_info.get('size', 0),
                            "type": file_type
                        })
                        total_size += file_info.get('size', 0)
            except Exception as e:
                log(f"‚ö†Ô∏è Failed to process files for process {process_id}: {e}", "WARN")
                continue
        
        response_data = {
            "download_items": download_items,
            "zip_url": None,  # TODO: Implement zip creation if needed
            "total_files": len(download_items),
            "total_size": total_size
        }
        
        return response_data
        
    except Exception as e:
        log(f"‚ùå Bulk download error: {e}", "ERROR")
        return {"error": f"Failed to create bulk download: {str(e)}"}

# Start RunPod Serverless
if __name__ == "__main__":
    log("üöÄ Starting LoRA Dashboard RunPod Serverless Handler", "INFO")
    runpod.serverless.start({
        "handler": async_handler,  # ‚úÖ Bezpo≈õrednio async handler
        "return_aggregate_stream": True
    }) 