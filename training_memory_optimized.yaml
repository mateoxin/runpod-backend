job: extension
config:
  name: "Matt_flux_lora_MEMORY_OPTIMIZED"    # Wersja zoptymalizowana pod kątem pamięci

  process:
    - type: sd_trainer
      training_folder: "/workspace/output"
      device: cuda:0
      trigger_word: "Matt"

      # LoRA network - ZREDUKOWANE dla oszczędności pamięci
      network:
        type: lora
        linear: 8                         # 🔽 ZREDUKOWANE z 20 → 8 (drastyczna oszczędność pamięci)
        linear_alpha: 8                   # Alpha = rank 
        network_dropout: 0.1              # Zwiększone dropout dla regularyzacji
        transformer_only: true

      # Zapisywanie - konserwatywne ustawienia
      save:
        dtype: float16
        save_every: 500                   # 🔽 Rzadziej zapisuj (oszczędność I/O)
        max_step_saves_to_keep: 2         # 🔽 Mniej checkpointów (oszczędność dysku)
        push_to_hub: false
        save_format: "safetensors"
        save_model_as: "safetensors"
        save_precision: "fp16"

      # Dataset - ZOPTYMALIZOWANE ścieżki
      datasets:
        - folder_path: "/workspace/training_data"
          caption_ext: "txt"
          caption_dropout_rate: 0.1       # Zwiększone dla regularyzacji
          shuffle_tokens: false
          cache_latents_to_disk: true     # ✅ Kluczowe dla oszczędności RAM
          resolution: [512]               # 🔽 TYLKO najniższa rozdzielczość!
          random_crop: false
          center_crop: true
          flip_p: 0.0
          pin_memory: false               # 🔽 Wyłączone dla oszczędności RAM
          num_workers: 1                  # 🔽 Mniej workers = mniej RAM
          persistent_workers: false       # 🔽 Wyłączone dla oszczędności

      # TRENING - MAKSYMALNE OSZCZĘDNOŚCI PAMIĘCI
      train:
        batch_size: 1                     # ✅ Minimalna wartość
        steps: 500                        # 🔽 Krótszy trening dla testu
        gradient_accumulation_steps: 2    # ✅ Symuluje większy batch bez RAM
        train_unet: true
        train_text_encoder: false         # ✅ Wyłączone dla oszczędności
        gradient_checkpointing: true      # ✅ Kluczowe dla oszczędności
        noise_scheduler: "flowmatch"
        optimizer: "adamw8bit"            # ✅ 8-bit optimizer
        lr: 0.0001                        # Standardowy learning rate
        lr_scheduler: "constant"
        # Precyzja - agresywna oszczędność
        dtype: "fp16"                     # 🔽 fp16 zamiast bf16 (mniej RAM)
        mixed_precision: "fp16"           # 🔽 fp16 dla wszystkiego
        # EMA - wyłączone dla oszczędności
        # ema_config:                     # 🔽 Całkowicie wyłączone
        #   use_ema: false
        # Podstawowe ustawienia
        max_grad_norm: 1.0
        skip_cache_check: true
        enable_xformers: true             # ✅ Memory-efficient attention
        compile_unet: false               # Wyłączone dla stabilności
        dataloader_num_workers: 1         # 🔽 Minimalne workers
        seed: 42
        noise_offset: 0.02                # 🔽 Zmniejszone

      # Model - MAKSYMALNE OSZCZĘDNOŚCI
      model:
        name_or_path: "black-forest-labs/FLUX.1-dev"
        is_flux: true
        quantize: true                    # ✅ Kluczowe
        low_vram: true                    # 🔽 WŁĄCZONE dla oszczędności!
        dtype: "fp16"                     # 🔽 fp16 zamiast bf16
        attention_mechanism: "xformers"   # ✅ Memory-efficient
        use_8bit_adam: true               # 🔽 WŁĄCZONE dla oszczędności
        gradient_checkpointing_text_encoder: true
        # NOWE - dodatkowe oszczędności
        enable_sequential_cpu_offload: true     # 🔽 CPU offload dla RAM
        enable_model_cpu_offload: true         # 🔽 Model na CPU gdy nie używany
        enable_attention_slicing: true         # 🔽 Slice attention dla RAM

      # Próbki - MINIMALNE dla oszczędności
      sample:
        sampler: "flowmatch"
        sample_every: 500                 # 🔽 Rzadsze sampling
        width: 512                        # 🔽 Najmniejsza rozdzielczość
        height: 512                       # 🔽 Kwadratowa dla oszczędności
        guidance_scale: 3.5               # Lekko zredukowane
        sample_steps: 15                  # 🔽 Mniej kroków dla szybkości
        seed: 42
        walk_seed: false                  # 🔽 Wyłączone dla oszczędności
        # Minimalne prompts
        prompts:
          - "photo of Matt"               # Tylko podstawowy prompt
        neg: ""
        enable_attention_slicing: true    # 🔽 Kluczowe dla RAM
        enable_sequential_cpu_offload: true  # 🔽 CPU offload